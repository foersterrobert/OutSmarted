{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgcbHr_eSPRB",
        "outputId": "f450156f-93bc-4a6e-89f5-bc53db7b6e41"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWDhr1EzNUg1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from random import shuffle\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1qszrsFNUg5",
        "outputId": "5eaa94b3-900c-4835-c71d-319c90fa5f43"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "numIters=128                                # Total number of training iterations\n",
        "num_simulations=15                          # Total number of MCTS simulations to run when deciding on a move to play\n",
        "numEps=100                                  # Number of full games (episodes) to run during each iteration\n",
        "numItersForTrainExamplesHistory=20\n",
        "num_epochs=2                                # Number of epochs of training per iteration\n",
        "learning_rate=5e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pezC0csNNUg6"
      },
      "outputs": [],
      "source": [
        "class Connect4Model(nn.Module):\n",
        "    def __init__(self, board_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                ConvBlock(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "                ConvBlock(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "                ResidualBlock(channels=128, num_repeats=4),\n",
        "                ConvBlock(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "                ResidualBlock(channels=64, num_repeats=4),\n",
        "                ConvBlock(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                ConvBlock(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "                nn.Flatten()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Two heads on our network\n",
        "        self.action_head = nn.Linear(in_features=board_size, out_features=action_size)\n",
        "        self.value_head = nn.Linear(in_features=board_size, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        action_logits = self.action_head(x)\n",
        "        value_logit = self.value_head(x)\n",
        "\n",
        "        return F.softmax(action_logits, dim=1), torch.tanh(value_logit)\n",
        "\n",
        "    def predict(self, board):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            pi, v = self.forward(board)\n",
        "\n",
        "        return pi.data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "        \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, num_repeats=1):\n",
        "        super().__init__()\n",
        "        self.num_repeats = num_repeats\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(self.num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    ConvBlock(channels, channels // 2, kernel_size=1),\n",
        "                    ConvBlock(channels // 2, channels, kernel_size=3, padding=1)\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91RWtGmONUg8"
      },
      "outputs": [],
      "source": [
        "def ucb_score(parent, child):\n",
        "    \"\"\"\n",
        "    The score for an action that would transition between the parent and child.\n",
        "    \"\"\"\n",
        "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "    if child.visit_count > 0:\n",
        "        # The value of the child is from the perspective of the opposing player\n",
        "        value_score = -child.value()\n",
        "    else:\n",
        "        value_score = 0\n",
        "\n",
        "    return value_score + prior_score\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior, to_play):\n",
        "        self.visit_count = 0\n",
        "        self.to_play = to_play\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.state = None\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def select_action(self, temperature):\n",
        "        \"\"\"\n",
        "        Select action according to the visit count distribution and the temperature.\n",
        "        \"\"\"\n",
        "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
        "        actions = [action for action in self.children.keys()]\n",
        "        if temperature == 0:\n",
        "            action = actions[np.argmax(visit_counts)]\n",
        "        elif temperature == float(\"inf\"):\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            # See paper appendix Data Generation\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
        "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
        "            action = np.random.choice(actions, p=visit_count_distribution)\n",
        "        return action\n",
        "\n",
        "    def select_child(self):\n",
        "        \"\"\"\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        best_score = -np.inf\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            score = ucb_score(self, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def expand(self, state, to_play, action_probs):\n",
        "        \"\"\"\n",
        "        We expand a node and keep track of the prior policy probability given by neural network\n",
        "        \"\"\"\n",
        "        self.to_play = to_play\n",
        "        self.state = state\n",
        "        for a, prob in enumerate(action_probs):\n",
        "            if prob != 0:\n",
        "                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Debugger pretty print node info\n",
        "        \"\"\"\n",
        "        prior = \"{0:.2f}\".format(self.prior)\n",
        "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, game, model):\n",
        "        self.game = game\n",
        "        self.model = model\n",
        "\n",
        "    def run(self, model, state, to_play):\n",
        "        root = Node(0, to_play)\n",
        "\n",
        "        # EXPAND root\n",
        "        action_probs, value = model.predict(torch.FloatTensor(state.reshape(1, 1, 6, 7).astype(np.float32)).to(device))\n",
        "        valid_moves = self.game.get_valid_moves(state)\n",
        "        action_probs = action_probs * valid_moves  # mask invalid moves\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        root.expand(state, to_play, action_probs)\n",
        "\n",
        "        for _ in range(num_simulations):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "\n",
        "            # SELECT\n",
        "            while node.expanded():\n",
        "                action, node = node.select_child()\n",
        "                search_path.append(node)\n",
        "\n",
        "            parent = search_path[-2]\n",
        "            state = parent.state\n",
        "            # Now we're at a leaf node and we would like to expand\n",
        "            # Players always play from their own perspective\n",
        "            next_state, _ = self.game.get_next_state(state, player=1, action=action)\n",
        "            # Get the board from the perspective of the other player\n",
        "            next_state = self.game.get_canonical_board(next_state, player=-1)\n",
        "\n",
        "            # The value of the new state from the perspective of the other player\n",
        "            value = self.game.get_reward_for_player(next_state, player=1)\n",
        "            if value is None:\n",
        "                # If the game has not ended:\n",
        "                # EXPAND\n",
        "                action_probs, value = model.predict(torch.FloatTensor(next_state.reshape(1, 1, 6, 7).astype(np.float32)).to(device))\n",
        "                valid_moves = self.game.get_valid_moves(next_state)\n",
        "                action_probs = action_probs * valid_moves  # mask invalid moves\n",
        "                action_probs /= np.sum(action_probs)\n",
        "                node.expand(next_state, parent.to_play * -1, action_probs)\n",
        "\n",
        "            self.backpropagate(search_path, value, parent.to_play * -1)\n",
        "\n",
        "        return root\n",
        "\n",
        "    def backpropagate(self, search_path, value, to_play):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value if node.to_play == to_play else -value\n",
        "            node.visit_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HPoNWbHNUg-"
      },
      "outputs": [],
      "source": [
        "class Connect4Game:\n",
        "    def __init__(self):\n",
        "        self.columns = 7\n",
        "        self.rows = 6\n",
        "\n",
        "    def get_init_board(self):\n",
        "        b = np.zeros((self.rows, self.columns,), dtype=int)\n",
        "        return b\n",
        "\n",
        "    def get_board_size(self):\n",
        "        return self.rows*self.columns\n",
        "\n",
        "    def get_action_size(self):\n",
        "        return self.columns\n",
        "\n",
        "    def get_next_state(self, board, player, action):\n",
        "        b = np.copy(board)\n",
        "        column_b = b[:, action]\n",
        "        non_zero = np.where(column_b != 0)[0]\n",
        "        if non_zero.size == 0:\n",
        "            i = self.rows - 1\n",
        "        else:\n",
        "            i = non_zero[0] - 1\n",
        "        b[i, action] = player\n",
        "        # Return the new game, but\n",
        "        # change the perspective of the game with negative\n",
        "        return (b, -player)\n",
        "\n",
        "    def has_legal_moves(self, board):\n",
        "        for index in range(self.columns):\n",
        "            if board[0, index] == 0:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_valid_moves(self, board):\n",
        "        # All moves are invalid by default\n",
        "        valid_moves = [0] * self.get_action_size()\n",
        "        for index in range(self.columns):\n",
        "            if board[0, index] == 0:\n",
        "                valid_moves[index] = 1\n",
        "        return valid_moves\n",
        "\n",
        "    def is_win(self, board, player):\n",
        "        for i in range(6):\n",
        "            for j in range(4):\n",
        "                if board[i, j] == board[i, j + 1] == board[i, j + 2] == board[i, j + 3] == player:\n",
        "                    return True\n",
        "        for i in range(3):\n",
        "            for j in range(7):\n",
        "                if board[i, j] == board[i + 1, j] == board[i + 2, j] == board[i + 3, j] == player:\n",
        "                    return True\n",
        "        for i in range(3):\n",
        "            for j in range(4):\n",
        "                if board[i, j] == board[i + 1, j + 1] == board[i + 2, j + 2] == board[i + 3, j + 3] == player:\n",
        "                    return True\n",
        "        for i in range(3, 6):\n",
        "            for j in range(4):\n",
        "                if board[i, j] == board[i - 1, j + 1] == board[i - 2, j + 2] == board[i - 3, j + 3] == player:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def get_reward_for_player(self, board, player):\n",
        "        if self.is_win(board, player):\n",
        "            return 1\n",
        "        if self.is_win(board, -player):\n",
        "            return -1\n",
        "        if self.has_legal_moves(board):\n",
        "            return None\n",
        "        return 0\n",
        "\n",
        "    def get_canonical_board(self, board, player):\n",
        "        return player * board"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeRrSaKxNUhA"
      },
      "outputs": [],
      "source": [
        "game = Connect4Game()\n",
        "board_size = game.get_board_size()\n",
        "action_size = game.get_action_size()\n",
        "\n",
        "model = Connect4Model(board_size, action_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tx5WttGQoaq"
      },
      "outputs": [],
      "source": [
        "def exceute_episode():\n",
        "    train_examples = []\n",
        "    current_player = 1\n",
        "    state = game.get_init_board()\n",
        "\n",
        "    while True:\n",
        "        canonical_board = game.get_canonical_board(state, current_player)\n",
        "\n",
        "        mcts = MCTS(game, model)\n",
        "        root = mcts.run(model, canonical_board, to_play=1)\n",
        "\n",
        "        action_probs = [0 for _ in range(game.get_action_size())]\n",
        "        for k, v in root.children.items():\n",
        "            action_probs[k] = v.visit_count\n",
        "\n",
        "        action_probs = action_probs / np.sum(action_probs)\n",
        "        train_examples.append((canonical_board, current_player, action_probs))\n",
        "\n",
        "        action = root.select_action(temperature=0)\n",
        "        state, current_player = game.get_next_state(state, current_player, action)\n",
        "        reward = game.get_reward_for_player(state, current_player)\n",
        "\n",
        "        if reward is not None:\n",
        "            ret = []\n",
        "            for hist_state, hist_current_player, hist_action_probs in train_examples:\n",
        "                # [Board, currentPlayer, actionProbabilities, Reward]\n",
        "                ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n",
        "            return ret\n",
        "\n",
        "def learn():\n",
        "    for i in range(1, numIters + 1):\n",
        "        print(\"{}/{}\".format(i, numIters))\n",
        "\n",
        "        train_examples = []\n",
        "        for eps in range(numEps):\n",
        "            iteration_train_examples = exceute_episode()\n",
        "            train_examples.extend(iteration_train_examples)\n",
        "\n",
        "        shuffle(train_examples)\n",
        "        train(train_examples)\n",
        "\n",
        "def train(examples):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    pi_losses = []\n",
        "    v_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "\n",
        "        batch_idx = 0\n",
        "        while batch_idx < int(len(examples) / batch_size):\n",
        "            sample_ids = np.random.randint(len(examples), size=batch_size)\n",
        "            boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
        "            boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
        "            target_pis = torch.FloatTensor(np.array(pis))\n",
        "            target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
        "\n",
        "            # predict\n",
        "            boards = boards.contiguous().to(device)\n",
        "            target_pis = target_pis.contiguous().to(device)\n",
        "            target_vs = target_vs.contiguous().to(device)\n",
        "\n",
        "            # compute output\n",
        "            out_pi, out_v = model(boards.reshape(-1, 1, 6, 7))\n",
        "            l_pi = loss_pi(target_pis, out_pi)\n",
        "            l_v = loss_v(target_vs, out_v)\n",
        "            total_loss = l_pi + l_v\n",
        "\n",
        "            pi_losses.append(float(l_pi))\n",
        "            v_losses.append(float(l_v))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        print()\n",
        "        print(\"Policy Loss\", np.mean(pi_losses))\n",
        "        print(\"Value Loss\", np.mean(v_losses))\n",
        "        print(\"Examples:\")\n",
        "        print(out_pi[0].detach())\n",
        "        print(target_pis[0])\n",
        "\n",
        "def loss_pi(targets, outputs):\n",
        "    loss = -(targets * torch.log(outputs)).sum(dim=1)\n",
        "    return loss.mean()\n",
        "\n",
        "def loss_v(targets, outputs):\n",
        "    loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xhJrZwTjSFp2",
        "outputId": "5495e13e-9a53-4c69-b2cf-fb1803848945"
      },
      "outputs": [],
      "source": [
        "learn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qNVG8GwSThn"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"drive/MyDrive/AlphaZero.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "alphaZero.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('myenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
