{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'numIters': 500,                                # Total number of training iterations\n",
    "    'num_simulations': 100,                         # Total number of MCTS simulations to run when deciding on a move to play\n",
    "    'numEps': 100,                                  # Number of full games (episodes) to run during each iteration\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "    'epochs': 2,                                    # Number of epochs of training per iteration\n",
    "    'checkpoint_path': 'latest.pth'                 # location to save latest set of weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Model(nn.Module):\n",
    "    def __init__(self, board_size, action_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                ConvBlock(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                ConvBlock(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "                ResidualBlock(channels=128, num_repeats=4),\n",
    "                ConvBlock(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                ResidualBlock(channels=64, num_repeats=4),\n",
    "                ConvBlock(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                ConvBlock(in_channels=32, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "                nn.Flatten()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Two heads on our network\n",
    "        self.action_head = nn.Linear(in_features=6*7, out_features=action_size)\n",
    "        self.value_head = nn.Linear(in_features=6*7, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        action_logits = self.action_head(x)\n",
    "        value_logit = self.value_head(x)\n",
    "\n",
    "        return F.softmax(action_logits, dim=1), torch.tanh(value_logit)\n",
    "\n",
    "    def predict(self, board):\n",
    "        board = torch.FloatTensor(board.astype(np.float32))\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.forward(board)\n",
    "\n",
    "        return pi.data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.num_repeats = num_repeats\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(self.num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(channels, channels // 2, kernel_size=1),\n",
    "                    ConvBlock(channels // 2, channels, kernel_size=3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior, to_play):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = to_play\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.state = None\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def select_action(self, temperature):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
    "        actions = [action for action in self.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "        return action\n",
    "\n",
    "    def select_child(self):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action = -1\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = ucb_score(self, child)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    def expand(self, state, to_play, action_probs):\n",
    "        \"\"\"\n",
    "        We expand a node and keep track of the prior policy probability given by neural network\n",
    "        \"\"\"\n",
    "        self.to_play = to_play\n",
    "        self.state = state\n",
    "        for a, prob in enumerate(action_probs):\n",
    "            if prob != 0:\n",
    "                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Debugger pretty print node info\n",
    "        \"\"\"\n",
    "        prior = \"{0:.2f}\".format(self.prior)\n",
    "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, model, args):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def run(self, model, state, to_play):\n",
    "        root = Node(0, to_play)\n",
    "\n",
    "        # EXPAND root\n",
    "        action_probs, value = model.predict(state.reshape(1, 1, 6, 7))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        root.expand(state, to_play, action_probs)\n",
    "\n",
    "        for _ in range(self.args['num_simulations']):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            # SELECT\n",
    "            while node.expanded():\n",
    "                action, node = node.select_child()\n",
    "                search_path.append(node)\n",
    "\n",
    "            parent = search_path[-2]\n",
    "            state = parent.state\n",
    "            # Now we're at a leaf node and we would like to expand\n",
    "            # Players always play from their own perspective\n",
    "            next_state, _ = self.game.get_next_state(state, player=1, action=action)\n",
    "            # Get the board from the perspective of the other player\n",
    "            next_state = self.game.get_canonical_board(next_state, player=-1)\n",
    "\n",
    "            # The value of the new state from the perspective of the other player\n",
    "            value = self.game.get_reward_for_player(next_state, player=1)\n",
    "            if value is None:\n",
    "                # If the game has not ended:\n",
    "                # EXPAND\n",
    "                action_probs, value = model.predict(next_state.reshape(1, 1, 6, 7))\n",
    "                valid_moves = self.game.get_valid_moves(next_state)\n",
    "                action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                node.expand(next_state, parent.to_play * -1, action_probs)\n",
    "\n",
    "            self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "\n",
    "        return root\n",
    "\n",
    "    def backpropagate(self, search_path, value, to_play):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connect4Game:\n",
    "    def __init__(self):\n",
    "        self.columns = 7\n",
    "        self.rows = 6\n",
    "\n",
    "    def get_init_board(self):\n",
    "        b = np.zeros((self.rows, self.columns,), dtype=int)\n",
    "        return b\n",
    "\n",
    "    def get_board_size(self):\n",
    "        return [self.rows, self.columns]\n",
    "\n",
    "    def get_action_size(self):\n",
    "        return self.columns\n",
    "\n",
    "    def get_next_state(self, board, player, action):\n",
    "        b = np.copy(board)\n",
    "        column_b = b[:, action]\n",
    "        non_zero = np.where(column_b != 0)[0]\n",
    "        if non_zero.size == 0:\n",
    "            i = self.rows - 1\n",
    "        else:\n",
    "            i = non_zero[0] - 1\n",
    "        b[i, action] = player\n",
    "        # Return the new game, but\n",
    "        # change the perspective of the game with negative\n",
    "        return (b, -player)\n",
    "\n",
    "    def has_legal_moves(self, board):\n",
    "        for index in range(self.columns):\n",
    "            if board[0, index] == 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_valid_moves(self, board):\n",
    "        # All moves are invalid by default\n",
    "        valid_moves = [0] * self.get_action_size()\n",
    "        for index in range(self.columns):\n",
    "            if board[0, index] == 0:\n",
    "                valid_moves[index] = 1\n",
    "        return valid_moves\n",
    "\n",
    "    def is_win(self, board, player):\n",
    "        for i in range(6):\n",
    "            for j in range(4):\n",
    "                if board[i, j] == board[i, j + 1] == board[i, j + 2] == board[i, j + 3] == player:\n",
    "                    return True\n",
    "        for i in range(3):\n",
    "            for j in range(7):\n",
    "                if board[i, j] == board[i + 1, j] == board[i + 2, j] == board[i + 3, j] == player:\n",
    "                    return True\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                if board[i, j] == board[i + 1, j + 1] == board[i + 2, j + 2] == board[i + 3, j + 3] == player:\n",
    "                    return True\n",
    "        for i in range(3, 6):\n",
    "            for j in range(4):\n",
    "                if board[i, j] == board[i - 1, j + 1] == board[i - 2, j + 2] == board[i - 3, j + 3] == player:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def get_reward_for_player(self, board, player):\n",
    "        if self.is_win(board, player):\n",
    "            return 1\n",
    "        if self.is_win(board, -player):\n",
    "            return -1\n",
    "        if self.has_legal_moves(board):\n",
    "            return None\n",
    "        return 0\n",
    "\n",
    "    def get_canonical_board(self, board, player):\n",
    "        return player * board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, game, model, args):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.model, self.args)\n",
    "\n",
    "    def exceute_episode(self):\n",
    "        train_examples = []\n",
    "        current_player = 1\n",
    "        state = self.game.get_init_board()\n",
    "\n",
    "        while True:\n",
    "            canonical_board = self.game.get_canonical_board(state, current_player)\n",
    "\n",
    "            self.mcts = MCTS(self.game, self.model, self.args)\n",
    "            root = self.mcts.run(self.model, canonical_board, to_play=1)\n",
    "\n",
    "            action_probs = [0 for _ in range(self.game.get_action_size())]\n",
    "            for k, v in root.children.items():\n",
    "                action_probs[k] = v.visit_count\n",
    "\n",
    "            action_probs = action_probs / np.sum(action_probs)\n",
    "            train_examples.append((canonical_board, current_player, action_probs))\n",
    "\n",
    "            action = root.select_action(temperature=0)\n",
    "            state, current_player = self.game.get_next_state(state, current_player, action)\n",
    "            reward = self.game.get_reward_for_player(state, current_player)\n",
    "\n",
    "            if reward is not None:\n",
    "                ret = []\n",
    "                for hist_state, hist_current_player, hist_action_probs in train_examples:\n",
    "                    # [Board, currentPlayer, actionProbabilities, Reward]\n",
    "                    ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n",
    "                return ret\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(1, self.args['numIters'] + 1):\n",
    "            print(\"{}/{}\".format(i, self.args['numIters']))\n",
    "\n",
    "            train_examples = []\n",
    "            for eps in range(self.args['numEps']):\n",
    "                iteration_train_examples = self.exceute_episode()\n",
    "                train_examples.extend(iteration_train_examples)\n",
    "\n",
    "            shuffle(train_examples)\n",
    "            self.train(train_examples)\n",
    "            filename = self.args['checkpoint_path']\n",
    "            self.save_checkpoint(folder=\".\", filename=filename)\n",
    "\n",
    "    def train(self, examples):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=5e-4)\n",
    "        pi_losses = []\n",
    "        v_losses = []\n",
    "\n",
    "        for epoch in range(self.args['epochs']):\n",
    "            self.model.train()\n",
    "\n",
    "            batch_idx = 0\n",
    "            while batch_idx < int(len(examples) / self.args['batch_size']):\n",
    "                sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n",
    "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                # predict\n",
    "                boards = boards.contiguous()\n",
    "                target_pis = target_pis.contiguous()\n",
    "                target_vs = target_vs.contiguous()\n",
    "\n",
    "                # compute output\n",
    "                out_pi, out_v = self.model(boards)\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                pi_losses.append(float(l_pi))\n",
    "                v_losses.append(float(l_v))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_idx += 1\n",
    "\n",
    "            print()\n",
    "            print(\"Policy Loss\", np.mean(pi_losses))\n",
    "            print(\"Value Loss\", np.mean(v_losses))\n",
    "            print(\"Examples:\")\n",
    "            print(out_pi[0].detach())\n",
    "            print(target_pis[0])\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        loss = -(targets * torch.log(outputs)).sum(dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
    "        return loss\n",
    "\n",
    "    def save_checkpoint(self, folder, filename):\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        torch.save({\n",
    "            'state_dict': self.model.state_dict(),\n",
    "        }, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Connect4Game()\n",
    "board_size = game.get_board_size()\n",
    "action_size = game.get_action_size()\n",
    "\n",
    "model = Connect4Model(board_size, action_size)\n",
    "\n",
    "trainer = Trainer(game, model, args)\n",
    "trainer.learn()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2177f1ca12c1330a133c1d40b46100b268ab447cddcbdfdc0c7b2b7e4840e700"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
